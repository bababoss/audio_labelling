{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from torch.distributed import get_rank\n",
    "from torch.distributed import get_world_size\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import torch\n",
    "import torchaudio\n",
    "import math,glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, 'blackman': scipy.signal.blackman,\n",
    "           'bartlett': scipy.signal.bartlett}\n",
    "\n",
    "\n",
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    \n",
    "    return sound\n",
    "\n",
    "\n",
    "def load_audio_scipy(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound\n",
    "\n",
    "def audio_with_noise(sample_path,noise_path):\n",
    "    sample=load_audio(sample_path)\n",
    "    p=\"/home/ubuntu/projects/datasets/noise_data_30_sec/23318.wav\"\n",
    "    noise=load_audio(noise_path)\n",
    "    if len(sample)<=len(noise):\n",
    "        data=sample+noise[:len(sample)]\n",
    "        return data\n",
    "#     else:\n",
    "#         noise=load_audio(p)\n",
    "#         #sample=np.pad(sample, (0, max(0, len(noise) - len(sample))), \"constant\")\n",
    "#         data=sample+noise\n",
    "#         return data\n",
    "    else:\n",
    "        #print(noise_path)\n",
    "        return sample\n",
    "\n",
    "class AudioParser(object):\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        \"\"\"\n",
    "        :param transcript_path: Path where transcript is stored from the manifest file\n",
    "        :return: Transcript in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        :param audio_path: Path where audio is stored from the manifest file\n",
    "        :return: Audio in training/testing format\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "\n",
    "class NoiseInjection(object):\n",
    "    def __init__(self,\n",
    "                 path=None,\n",
    "                 sample_rate=16000,\n",
    "                 noise_levels=(0, 0.5)):\n",
    "        \"\"\"\n",
    "        Adds noise to an input signal with specific SNR. Higher the noise level, the more noise added.\n",
    "        Modified code from https://github.com/willfrey/audio/blob/master/torchaudio/transforms.py\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(\"Directory doesn't exist: {}\".format(path))\n",
    "            raise IOError\n",
    "        self.paths = path is not None and librosa.util.find_files(path)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.noise_levels = noise_levels\n",
    "\n",
    "    def inject_noise(self, data):\n",
    "        noise_path = np.random.choice(self.paths)\n",
    "        noise_level = np.random.uniform(*self.noise_levels)\n",
    "        return self.inject_noise_sample(data, noise_path, noise_level)\n",
    "\n",
    "    def inject_noise_sample(self, data, noise_path, noise_level):\n",
    "        noise_len = get_audio_length(noise_path)\n",
    "        noise_data = load_audio(noise_path)\n",
    "\n",
    "        data_len = len(data) / self.sample_rate\n",
    "        if len(data) < len(noise_data):\n",
    "            data=data+noise_data[:len(data)]\n",
    "        else:\n",
    "            print(noise_path)\n",
    "#         noise_start = np.random.rand() * (noise_len - data_len)\n",
    "#         noise_end = noise_start + data_len\n",
    "#         noise_dst = audio_with_sox(noise_path, self.sample_rate, noise_start, noise_end)\n",
    "#         assert len(data) == len(noise_dst)\n",
    "#         noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)\n",
    "#         data_energy = np.sqrt(data.dot(data) / data.size)\n",
    "#         data += noise_level * noise_dst * data_energy / noise_energy\n",
    "        return data\n",
    "\n",
    "\n",
    "class SpectrogramParser(AudioParser):\n",
    "    def __init__(self, audio_conf, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Parses audio file into spectrogram with optional normalization and various augmentations\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        super(SpectrogramParser, self).__init__()\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.sample_rate = audio_conf['sample_rate']\n",
    "        self.window = windows.get(audio_conf['window'], windows['hamming'])\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.noiseInjector = NoiseInjection(audio_conf['noise_dir'], self.sample_rate,\n",
    "                                            audio_conf['noise_levels']) if audio_conf.get(\n",
    "            'noise_dir') is not None else None\n",
    "        self.noise_prob = audio_conf.get('noise_prob')\n",
    "        self.noise_list=glob.glob(audio_conf['noise_dir']+\"/*.wav\")\n",
    "\n",
    "    def parse_audio(self, audio_path):\n",
    "#         if self.augment:\n",
    "#             y = load_randomly_augmented_audio(audio_path, self.sample_rate)\n",
    "#             print(\"augument: \")\n",
    "#         else:\n",
    "#             #print(audio_path)\n",
    "#             y = load_audio(audio_path)\n",
    "        # original target \n",
    "        target = load_audio(audio_path)\n",
    "        #y = load_audio(audio_path)\n",
    "        \n",
    "        n_fft = int(self.sample_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sample_rate * self.window_stride)\n",
    "        \n",
    "        if self.noiseInjector:\n",
    "#             add_noise = np.random.binomial(1, self.noise_prob)\n",
    "#             if add_noise:\n",
    "            y = self.noiseInjector.inject_noise(target)\n",
    "#             y=audio_with_noise(audio_path,random.choice(self.noise_list))\n",
    "            #print(\"noise_add\")\n",
    "            # STFT  and NOISE SPECT \n",
    "            D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                             win_length=win_length, window=self.window)\n",
    "            spect, phase = librosa.magphase(D)\n",
    "            # S = log(S+1)\n",
    "            spect = np.log1p(spect)\n",
    "            spect = torch.FloatTensor(spect)\n",
    "            if self.normalize:\n",
    "                mean = spect.mean()\n",
    "                std = spect.std()\n",
    "                spect.add_(-mean)\n",
    "                spect.div_(std)\n",
    "#             else:\n",
    "#                 print(\"ADD Noise failed\")\n",
    "\n",
    "        else:\n",
    "            print(\"Please add noise dir path\")\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "        # TARGET SPECT\n",
    "        Target_D = librosa.stft(target, n_fft=n_fft, hop_length=hop_length,\n",
    "                         win_length=win_length, window=self.window)\n",
    "        target_spect, target_phase = librosa.magphase(Target_D)\n",
    "        # S = log(S+1)\n",
    "        target_spect = np.log1p(target_spect)\n",
    "        target_spect = torch.FloatTensor(target_spect)\n",
    "        if self.normalize:\n",
    "            mean = target_spect.mean()\n",
    "            std = target_spect.std()\n",
    "            target_spect.add_(-mean)\n",
    "            target_spect.div_(std)\n",
    "\n",
    "        #return spect\n",
    "        #print(\"y::::--------\",torch.FloatTensor(y).shape)\n",
    "        #print(\"y::::--------\",spect.shape)\n",
    "#         sample_rate, samples = wavfile.read(audio_path)\n",
    "#         frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "        #spect,target_spect=spect[:80,:36],target_spect[:80,:36]\n",
    "        #print(spect[:80,:])\n",
    "        #print(spect.shape,target_spect.shape)\n",
    "        h,w=target_spect.shape[0],target_spect.shape[1]\n",
    "        target_spect=target_spect[:(h-h%4),:(w-w%4)]\n",
    "        spect=spect[:(h-h%4),:(w-w%4)]\n",
    "        #print(spect.shape,target_spect.shape)\n",
    "        return spect,target_spect#torch.FloatTensor(spectrogram)\n",
    "    \n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset, SpectrogramParser):\n",
    "    def __init__(self, audio_conf, manifest_filepath, labels, normalize=False, augment=False):\n",
    "        \"\"\"\n",
    "        Dataset that loads tensors via a csv containing file paths to audio files and transcripts separated by\n",
    "        a comma. Each new line is a different sample. Example below:\n",
    "\n",
    "        /path/to/audio.wav,/path/to/audio.txt\n",
    "        ...\n",
    "\n",
    "        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds\n",
    "        :param manifest_filepath: Path to manifest csv as describe above\n",
    "        :param labels: String containing all the possible characters to map to\n",
    "        :param normalize: Apply standard mean and deviation normalization to audio tensor\n",
    "        :param augment(default False):  Apply random tempo and gain perturbations\n",
    "        \"\"\"\n",
    "        with open(manifest_filepath) as f:\n",
    "            ids = f.readlines()\n",
    "        ids = [x.strip().split(',') for x in ids[:2000]]\n",
    "        self.ids = ids\n",
    "        self.size = len(ids)\n",
    "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
    "        super(SpectrogramDataset, self).__init__(audio_conf, normalize, augment)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.ids[index]\n",
    "        audio_path, transcript_path = sample[0], sample[1]\n",
    "        spect,target_spect = self.parse_audio(audio_path)\n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "        return spect, transcript,target_spect\n",
    "\n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            transcript = transcript_file.read().replace('\\n', '')\n",
    "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    def func(p):\n",
    "        return p[0].size(1)\n",
    "\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "    longest_sample = max(batch, key=func)[0]\n",
    "    freq_size = longest_sample.size(0)\n",
    "    minibatch_size = len(batch)\n",
    "    max_seqlength = longest_sample.size(1)\n",
    "    inputs = torch.zeros(minibatch_size, 1, freq_size, max_seqlength)\n",
    "    \n",
    "    batch1 = sorted(batch, key=lambda sample: sample[2].size(1), reverse=True)\n",
    "    longest_sample1 = max(batch1, key=func)[0]\n",
    "    freq_size1 = longest_sample1.size(0)\n",
    "    minibatch_size1 = len(batch1)\n",
    "    max_seqlength1 = longest_sample1.size(1)\n",
    "    target_spects = torch.zeros(minibatch_size1, 1, freq_size1, max_seqlength1)\n",
    "    \n",
    "    input_percentages = torch.FloatTensor(minibatch_size)\n",
    "    target_sizes = torch.IntTensor(minibatch_size)\n",
    "    targets = []\n",
    "#     for x in range(minibatch_size):\n",
    "#         sample = batch[x]\n",
    "#         tensor = sample[0]\n",
    "#         target = sample[2]\n",
    "#         seq_length = tensor.size(1)\n",
    "#         inputs[x][0].narrow(1, 0, seq_length).copy_(tensor)\n",
    "#         input_percentages[x] = seq_length / float(max_seqlength)\n",
    "#         target_sizes[x] = len(target)\n",
    "#         targets.extend(target)\n",
    "    targets = torch.IntTensor(targets)\n",
    "    return inputs,target_spects, targets, input_percentages, target_sizes\n",
    "\n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a data loader for AudioDatasets.\n",
    "        \"\"\"\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn\n",
    "\n",
    "\n",
    "class BucketingSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=1):\n",
    "        \"\"\"\n",
    "        Samples batches assuming they are in order of size to batch similarly sized samples together.\n",
    "        \"\"\"\n",
    "        super(BucketingSampler, self).__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        ids = list(range(0, len(data_source)))\n",
    "        self.bins = [ids[i:i + batch_size] for i in range(0, len(ids), batch_size)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ids in self.bins:\n",
    "            np.random.shuffle(ids)\n",
    "            yield ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bins)\n",
    "\n",
    "    def shuffle(self, epoch):\n",
    "        np.random.shuffle(self.bins)\n",
    "\n",
    "\n",
    "class DistributedBucketingSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=1, num_replicas=None, rank=None):\n",
    "        \"\"\"\n",
    "        Samples batches assuming they are in order of size to batch similarly sized samples together.\n",
    "        \"\"\"\n",
    "        super(DistributedBucketingSampler, self).__init__(data_source)\n",
    "        if num_replicas is None:\n",
    "            num_replicas = get_world_size()\n",
    "        if rank is None:\n",
    "            rank = get_rank()\n",
    "        self.data_source = data_source\n",
    "        self.ids = list(range(0, len(data_source)))\n",
    "        self.batch_size = batch_size\n",
    "        self.bins = [self.ids[i:i + batch_size] for i in range(0, len(self.ids), batch_size)]\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.num_samples = int(math.ceil(len(self.bins) * 1.0 / self.num_replicas))\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "\n",
    "    def __iter__(self):\n",
    "        offset = self.rank\n",
    "        # add extra samples to make it evenly divisible\n",
    "        bins = self.bins + self.bins[:(self.total_size - len(self.bins))]\n",
    "        assert len(bins) == self.total_size\n",
    "        samples = bins[offset::self.num_replicas]  # Get every Nth bin, starting from rank\n",
    "        return iter(samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def shuffle(self, epoch):\n",
    "        # deterministically shuffle based on epoch\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(epoch)\n",
    "        bin_ids = list(torch.randperm(len(self.bins), generator=g))\n",
    "        self.bins = [self.bins[i] for i in bin_ids]\n",
    "\n",
    "\n",
    "def get_audio_length(path):\n",
    "    output = subprocess.check_output(['soxi -D \\\"%s\\\"' % path.strip()], shell=True)\n",
    "    return float(output)\n",
    "\n",
    "\n",
    "def audio_with_sox(path, sample_rate, start_time, end_time):\n",
    "    \"\"\"\n",
    "    crop and resample the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as tar_file:\n",
    "        tar_filename = tar_file.name\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} trim {} ={} >/dev/null 2>&1\".format(path, sample_rate,\n",
    "                                                                                               tar_filename, start_time,\n",
    "                                                                                               end_time)\n",
    "        os.system(sox_params)\n",
    "        y = load_audio(tar_filename)\n",
    "        return y\n",
    "\n",
    "\n",
    "def augment_audio_with_sox(path, sample_rate, tempo, gain):\n",
    "    \"\"\"\n",
    "    Changes tempo and gain of the recording with sox and loads it.\n",
    "    \"\"\"\n",
    "    with NamedTemporaryFile(suffix=\".wav\") as augmented_file:\n",
    "        augmented_filename = augmented_file.name\n",
    "        sox_augment_params = [\"tempo\", \"{:.3f}\".format(tempo), \"gain\", \"{:.3f}\".format(gain)]\n",
    "        sox_params = \"sox \\\"{}\\\" -r {} -c 1 -b 16 -e si {} {} >/dev/null 2>&1\".format(path, sample_rate,\n",
    "                                                                                      augmented_filename,\n",
    "                                                                                      \" \".join(sox_augment_params))\n",
    "        os.system(sox_params)\n",
    "        y = load_audio(augmented_filename)\n",
    "#         os.remove(augmented_filename)\n",
    "        return y\n",
    "\n",
    "\n",
    "def load_randomly_augmented_audio(path, sample_rate=16000, tempo_range=(0.85, 1.15),\n",
    "                                  gain_range=(-6, 8)):\n",
    "    \"\"\"\n",
    "    Picks tempo and gain uniformly, applies it to the utterance by using sox utility.\n",
    "    Returns the augmented utterance.\n",
    "    \"\"\"\n",
    "    low_tempo, high_tempo = tempo_range\n",
    "    tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)\n",
    "    low_gain, high_gain = gain_range\n",
    "    gain_value = np.random.uniform(low=low_gain, high=high_gain)\n",
    "    audio = augment_audio_with_sox(path=path, sample_rate=sample_rate,\n",
    "                                   tempo=tempo_value, gain=gain_value)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json,os\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "with open('/home/ubuntu/projects/deepspeech.pytorch/labels.json') as label_file:\n",
    "    labels = str(''.join(json.load(label_file)))\n",
    "\n",
    "audio_conf = dict(sample_rate=16000,\n",
    "                  window_size=.01,\n",
    "                  window_stride=.02,\n",
    "                  window='hamming',\n",
    "                  noise_dir=\"/home/ubuntu/projects/datasets/noise_data_30_sec\",\n",
    "                  noise_prob=.4,\n",
    "                  noise_levels=(0.0, 0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conf = dict(sample_rate=16000,\n",
    "                  window_size=.01,\n",
    "                  window_stride=.02,\n",
    "                  window='hamming',\n",
    "                  noise_dir=\"/home/ubuntu/projects/datasets/noise_data_30_sec\",\n",
    "                  noise_prob=.4,\n",
    "                  noise_levels=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest=\"/home/ubuntu/projects/deepspeech.pytorch/data/libri_train_manifest.csv\"\n",
    "val_manifest=\"/home/ubuntu/projects/deepspeech.pytorch/data/libri_test_clean_manifest.csv\"\n",
    "train_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath=train_manifest, labels=labels,\n",
    "                                   normalize=True, augment=True)\n",
    "test_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath=val_manifest, labels=labels,\n",
    "                                  normalize=True, augment=False)\n",
    "\n",
    "\n",
    "train_sampler = BucketingSampler(train_dataset, batch_size=32)\n",
    "train_loader = AudioDataLoader(train_dataset,\n",
    "                               num_workers=4, batch_sampler=train_sampler)\n",
    "test_loader = AudioDataLoader(test_dataset, batch_size=2,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5,stride=1,padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "#             nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8, 16, kernel_size=5,stride=1,padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "#             nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #nn.Conv2d(16,16,3,padding=(1,1)),\n",
    "           nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "                     \n",
    "        self.decoder = nn.Sequential(\n",
    "            #F.interpolate(mode='bilinear', scale_factor=2),\n",
    "            nn.ConvTranspose2d(16,8,kernel_size=2,stride=2),\n",
    "            nn.ReLU(True),\n",
    "            #F.interpolate(mode='bilinear', scale_factor=2),\n",
    "            nn.ConvTranspose2d(8,1,kernel_size=2,stride = 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        )\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(1, 2, kernel_size=3, stride=1,\n",
    "#             nn.BatchNorm2d(2),\n",
    "#             nn.Hardtanh(0, 20, inplace=True),\n",
    "#             nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Hardtanh(0, 20, inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.decoder = nn.Sequential(             \n",
    "#             nn.ConvTranspose2d(32,32, kernel_size=(41, 11)),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.ConvTranspose2d(32,1, kernel_size=(41, 11)),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        #print(x.shape)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size=(2,2)\n",
    "in_height,in_width=80, 24\n",
    "strides=(1,1)\n",
    "out_height,out_width=81,27\n",
    "\n",
    "padding_height = [strides[0] * (in_height - 1) + kernel_size[0] - out_height] \n",
    "padding_width  = [strides[1] * (in_width - 1) + kernel_size[1] - out_width] \n",
    "padding_height[0],padding_width[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining some params\n",
    "num_epochs = 1 #you can go for more epochs, I am using a mac\n",
    "if torch.cuda.is_available():\n",
    "    model = autoencoder().cuda()\n",
    "else:\n",
    "    model = autoencoder().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (1, 81, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:0.0005272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type autoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Use cuda: \")\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        inputs,target_spects, targets, input_percentages, target_sizes = data\n",
    "        #print(inputs.shape)\n",
    "        \n",
    "        #print(inputs.shape)\n",
    "        if torch.cuda.is_available():\n",
    "            input_spect = Variable(inputs).cuda()\n",
    "            target_spect = Variable(target_spects).cuda()\n",
    "        else:\n",
    "            input_spect = Variable(inputs).cpu()\n",
    "            target_spect = Variable(target_spects).cpu()\n",
    "\n",
    "        # ===================forward=====================\n",
    "        output = model(input_spect)\n",
    "        #print(\"output: \",output.shape,target_spect.shape)\n",
    "        #print(\"target_spect: \",target_spect.shape)\n",
    "        loss = distance(output, target_spect)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.7f}'.format(epoch+1, num_epochs, loss.data.cpu().numpy()))\n",
    "    torch.save(model, \"/home/ubuntu/projects/audioNoiseCanceller_Pytorch/saved_model/anc_model\"+str(epoch)+\".pth\")\n",
    "#     print('epoch [{}]'.format(epoch+1))\n",
    "#     print('_numepoch [{}]'.format(num_epochs))\n",
    "#     print('_numepoch [{}]'.format(loss.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    inputs,target_spects, targets, input_percentages, target_sizes = data\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape, target_spects.shape, targets.shape,input_percentages.shape, target_sizes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1, 81, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "au_path=\"/home/ubuntu/projects/deepspeech.pytorch/data/LibriSpeech_dataset/train/wav/4013-182396-0023.wav\"\n",
    "#au_path=\"/home/ubuntu/projects/deepspeech.pytorch/data/LibriSpeech_dataset/train/wav/2790-142824-0006.wav\"\n",
    "sample_rate, samples = wavfile.read(au_path)\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "print(samples.shape)\n",
    "print(spectrogram.shape)\n",
    "plt.pcolormesh(times, frequencies, spectrogram)\n",
    "plt.imshow(spectrogram)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    sound, _ = torchaudio.load(path, normalization=True)\n",
    "    sound = sound.numpy().T\n",
    "    if len(sound.shape) > 1:\n",
    "        if sound.shape[1] == 1:\n",
    "            sound = sound.squeeze()\n",
    "        else:\n",
    "            sound = sound.mean(axis=1)  # multiple channels, average\n",
    "    return sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_audio(au_path).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_with_noise(sample_path,noise_path):\n",
    "    sample=load_audio(sample_path)\n",
    "    noise=load_audio(noise_path)\n",
    "    if len(sample)<=len(noise):\n",
    "        data=sample+noise[:len(sample)]\n",
    "        return data\n",
    "    else:\n",
    "        sample=np.pad(sample, (0, max(0, len(noise) - len(sample))), \"constant\")\n",
    "        data=sample+noise\n",
    "        return data\n",
    "\n",
    "n_data=audio_with_noise(au_path,\"/home/ubuntu/projects/datasets/noise_data_30_sec/23318.wav\")\n",
    "n_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, _ = torchaudio.load(au_path, normalization=True)\n",
    "sound = sound.numpy().T\n",
    "sound.shape[1]\n",
    "sound = sound.squeeze()\n",
    "sound.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(sound, rate=16000) # load a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(n_data, rate=16000) # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/ubuntu/projects/datasets/noise_data_30_sec\"\n",
    "paths = path is not None and librosa.util.find_files(\"/home/ubuntu/projects/datasets/noise_data_30_sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "almondaiEnv",
   "language": "python",
   "name": "almondaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
